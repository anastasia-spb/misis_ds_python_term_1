{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4c593c",
   "metadata": {},
   "source": [
    "## PYTHON-PROJECT-2\n",
    "\n",
    "---\n",
    "\n",
    "Project 2 states the question: How to calculate term (word) importance in the text. In other words: the \"contrast\" of a word in a document (how much it stands out from other words).\n",
    "\n",
    "For that basic text analysis metrics such as **tf-idf** is introduced.\n",
    "\n",
    "**df** - document frequency\n",
    "\n",
    "**idf** - inverse document frequency\n",
    "\n",
    "$$ df = \\frac{d}{N} $$\n",
    "\n",
    "where\n",
    "\n",
    "**N:** total number of documents in the corpus (e.g. chapters in book);\n",
    "\n",
    "**d:** number of documents where the term t appears (e.g. in how many chapters the term t appears).\n",
    "\n",
    "---\n",
    "\n",
    "**tf** - term frequency\n",
    "\n",
    "$$ tf = \\frac{f}{M}$$\n",
    "\n",
    "where\n",
    "\n",
    "**f:** the raw count of a term in a document (e.g. how many times term t appears in chapter);\n",
    "\n",
    "**M:** the raw count of all terms in a document (e.g. how many words in chapter).\n",
    "\n",
    "---\n",
    "\n",
    "In the implementation below instead of raw ``tf * idf`` value,\n",
    "following formula is used: $$  \\log{1+tf} * \\log{idf)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc63ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Task: Write code that will print the three words with the highest **tf-idf** value in the given target_chapter \n",
    "chapter in descending tf-idf order, separated by a space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407ae470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "\n",
    "def read_data():\n",
    "    data = open('war_peace_processed.txt', 'rt').read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "def create_dict(input_data):\n",
    "    words_dict = {}\n",
    "    for word in input_data:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "    return words_dict\n",
    "\n",
    "def create_chapters_dicts(data):\n",
    "    chapters_dicts = []\n",
    "    chapters_lengths = []\n",
    "    deliminators_indicis = [i for i, e in enumerate(data[1:]) if e == '[new chapter]']\n",
    "    start = 0\n",
    "    for index in deliminators_indicis:\n",
    "        chapters_dicts.append(create_dict(data[start:index]))\n",
    "        chapters_lengths.append(len(data[start:index]))\n",
    "        start = index\n",
    "    # add last chapter\n",
    "    chapters_dicts.append(create_dict(data[start:]))\n",
    "    chapters_lengths.append(len(data[start:]))\n",
    "    return chapters_dicts, chapters_lengths\n",
    "\n",
    "def get_term_frequency(chapters_dicts, chapters_lengths, target_word, target_chapter):\n",
    "    return chapters_dicts[target_chapter].get(target_word, 0) / chapters_lengths[target_chapter]\n",
    "\n",
    "def get_document_frequency(chapters_dict, target_word):\n",
    "    frequency = 0\n",
    "    for chapter_dict in chapters_dict:\n",
    "        if target_word in chapter_dict:\n",
    "            frequency += 1\n",
    "    return frequency / len(chapters_dict)\n",
    "\n",
    "def get_tf_idf(chapters_dicts, chapters_lengths, target_word, target_chapter):\n",
    "    df = get_document_frequency(chapters_dicts, target_word)\n",
    "    tf = get_term_frequency(chapters_dicts, chapters_lengths, target_word, target_chapter)\n",
    "    idf = 1. / df\n",
    "    return math.log(1 + tf) * math.log(idf)\n",
    "\n",
    "def get_tf_idf_for_all(data, target_chapter, number):\n",
    "    # Find start and end index for target chapter\n",
    "    deliminators_indicis = [i for i, e in enumerate(data) if e == '[new chapter]']\n",
    "    deliminators_indicis.insert(0, 0)\n",
    "    chapter_start_idx = deliminators_indicis[target_chapter] + 1\n",
    "    chapter_end_idx = deliminators_indicis[target_chapter + 1]\n",
    "    # Create dict for every chapter\n",
    "    chapters_dicts, chapters_lengths = create_chapters_dicts(data)\n",
    "    # Calculate tf-idf for every word on chapter\n",
    "    tf_idf_values = {}\n",
    "    for target_word in chapters_dicts[target_chapter].keys():\n",
    "        tf_idf_values[target_word] = get_tf_idf(chapters_dicts, chapters_lengths, target_word, target_chapter)\n",
    "    return heapq.nlargest(number, tf_idf_values.items(), key=lambda item: item[1])\n",
    "\n",
    "def print_words_with_highest_tf_idf(data, target_chapter_idx, number=3):\n",
    "    words_with_highest_tf_idf = [result[0] for result in get_tf_idf_for_all(data, target_chapter_idx, number)] \n",
    "    print(*words_with_highest_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0ac946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "павловна анна тетушку\n"
     ]
    }
   ],
   "source": [
    "target_chapter_idx = 4\n",
    "data = read_data()\n",
    "print_words_with_highest_tf_idf(data, target_chapter_idx, number=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd3fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
